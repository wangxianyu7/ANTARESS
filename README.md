# **A**dvanced and **N**eat **T**echniques for the **A**ccurate **R**etrieval of **E**xoplanetary and **S**tellar **S**pectra

## General approach

First, define the system properties for the host star and its planets in [ANTARESS_systems.py](ANTARESS_systems.py).  
Then, define your input datasets, and the modules to process and analyze them, in a copy of the default configuration file [ANTARESS_settings.py](ANTARESS_settings.py). You can name your own configuration file as `ANTARESS_settings_username.py`.  
Finally, launch the workflow with the python command `ANTARESS_launcher(user = username)`. 

Most fields in the configuration file have default values. However, we recommend you try out changing their values to process your specific datasets.

Modules are grouped in three main categories:
- [ ] Data first go through formatting and correction modules, some of which are specific to given instruments. Once data are set in the common ANTARESS format and corrected for instrumental/environmental effects, they can be processed in the same way by the subsequent modules. 
- [ ] The second group of modules are thus generic and aim at extracting specific types of spectral profiles, converting them in the format required for the analysis chosen by the user. 
- [ ] The third group of modules allow fitting the processed spectral profiles to derive quantities of interest. 

Modules in the first and second group are ran successively, ie that data need to be processed by an earlier module before it can be used by the next one. 

Analyses modules, in contrast, are applied to the outputs of various processing modules throughout the pipeline. 

Some of the correction and processing modules are optional, for example the telluric correction module for space-borne data or the flux scaling module for data with absolute photometry. 

Some modules are only activated if the pipeline is used for a specific goal, for example the conversion of stellar spectra into CCFs when the user requires the analysis of the Rossiter-McLaughlin effect.

Each module can be activated independently 
- in most modules you can choose to calculate data (in which case it will then be automatically saved on disk) or to retrieve it (in which case the pipeline will check these data already exists)
- keeping all data in memory is not possible when processing e2ds, which is why the pipeline works in each module by retrieving the relevant data from the disk

## Plots

Plots are generated at the end of the workflow processing, upon request.

At the end of each module settings in the configuration file, you can activate a given `plot_name` by setting `plot_dic['plot_name']` to a figure extension, such as `pdf`.

The plot settings are then controlled through the plot configuration file, `ANTARESS_plot_settings.py`.

Because plots may require specific outputs of large size, the latter are not generated by default. This means you need to activate the plot as described above for the workflow to save the required outputs when the module is ran.

## Notebooks

Notebooks allows you to run a simplified version of the ANTARESS workflow. Their purpose is to help you familiarize with ANTARESS. To use the workflow to its full capabilities, user the configuration file.  

Notebooks available in the namesake directory:
- `ANTARESS_nbook_mock`: to generate a mock CCF dataset

## Required packages
- [ ] run `pip install` with [scipy](https://scipy.org/), lmfit, batman-package, astropy, emcee, pathos, pandas, dace_query, statsmodels, PyAstronomy        
- [ ] resampling package 
- [bindensity documentation](https://obswww.unige.ch/~delisle/staging/bindensity/doc/)
- run `pip install --extra-index-url https://vincent:cestpasfaux@obswww.unige.ch/~delisle/staging bindensity --upgrade`
- do not use routines with non-continuous tables, as it will mess up with banded covariance matrixes
- beware of masking ranges, as undefined pixels (set to nan values) are propagated when resampling or combining profiles in the various pipeline modules.
- [ ] pySME
- install gcc9 with `brew install gcc@9`
- run `pip install pysme-astro`
- [ ] KitCat
- install [gsl](https://www.gnu.org/software/gsl/) with `brew install gsl`
- run `python setup_lbl_fit.py build` after setting up the path to your local python installation in this file. Then copy the compiled file `calculate_RV_line_by_line3.cpython-XX-darwin.so` into your ANTARESS/KitCat directory  

## Contributors
**Main developer**: Vincent Bourrier (you can contact me for inquiries about the ANTARESS workflow and associated pipeline).  
**Contributors**: Omar Attia, Romain Allart, Khaled Al Moulla, Heather Cegla, Michaël Cretignier, William Dethier, Jean-Baptiste Delisle, Xavier Dumusque, David Ehrenreich, Nathan Hara, Christophe Lovis, Samson Jules Mercier, Dany Mounzer, Francesco Alfonso Pepe, Julia Victoria Seidel, Michal Steiner, Sara Tavella, Valentina Vaulato, Theo Vrignaud, Aurélien Wyttenbach.
